Phase 1: Core Infrastructure & User Experience

In Phase 1, the focus is on building a solid foundation for the application and ensuring a smooth user experience. This includes setting up user management and designing the interface, with an eye towards performance (especially low latency) from the very beginning. Key tasks and considerations in this phase:
	•	User Accounts & Project Management: Implement a system for user authentication and accounts. Each user should be able to create and manage multiple projects (groupings of songs or sessions). This involves setting up a database for storing user profiles, project info, and references to uploaded audio. Secure sign-up/login, password management, and possibly social login (OAuth) should be included. Organizing work into projects will help users keep track of different songs or sessions they analyze with the tool.
	•	Audio Upload & Management UI: Develop a full web-based user interface that allows users to upload audio files (e.g. MP3, WAV) and view the outputs (separated tracks, transcriptions, etc.). The UI should be intuitive, with features like drag-and-drop upload, progress indicators, and a list or library of the user’s uploaded tracks. Provide controls to play back original audio and any processed results. Designing a clean layout for viewing sheet music, MIDI playback controls, and stem toggles (mute/unmute instrument stems) will greatly enhance usability. Consider using a modern web framework for the frontend (React, Vue, etc.) to manage state and create a responsive experience.
	•	Low-Latency Architecture (Web Audio & WebAssembly): Right from the start, architect the audio processing pipeline for minimal latency. On the client side, use the Web Audio API for playback and real-time audio processing; this API allows audio scheduling and manipulation directly in the browser with low jitter. For heavy DSP (digital signal processing) tasks, consider using Rust or C++ to write high-performance code and compile it to WebAssembly (WASM) for use in the browser. WebAssembly enables near-native execution speeds in web applications ￼, which is crucial for real-time audio. By leveraging an AudioWorklet (a Web Audio API feature for running custom audio processing code in a dedicated audio thread) compiled from Rust/WASM, the system can handle DSP tasks with minimal delay. If machine learning inference is done client-side, frameworks like TensorFlow.js with WebGL or WebGPU acceleration can be explored to utilize the GPU. Overall, the backend and frontend should be designed to stream data efficiently and avoid any large latency spikes, laying the groundwork for the real-time features in later phases.

Phase 2: Audio Processing & Transcription

Phase 2 introduces the core audio analysis capabilities. The system will gain the ability to deconstruct a mixed music track into individual instruments and transcribe the music into MIDI/notation. We will integrate existing open-source models and tools to achieve these features, tailoring them to the jazz domain where needed. Key features in this phase:
	•	Source Separation into Stems: Integrate an open-source music source separation model to split songs into isolated instrument tracks (stems). For example, Demucs is a state-of-the-art model that can separate a music track into components like vocals, bass, drums, and other accompaniment ￼. We can incorporate Demucs (or a similar model) to allow users to upload a mixed track and get back individual audio stems for each instrument. This could be done server-side (due to heavy computation), but the results should be presented in the UI for playback and download. Alternatively, we can explore a flow-based generative model (such as a GLOW-based approach) for source separation, which involves training an invertible neural network for each source to act as a prior and then separating via probabilistic inference ￼. Using a flow-based model is research-intensive but could yield high-quality separations. We might start with a known model like Demucs for initial functionality and keep the architecture open for plugging in improved or alternate separation models (like Meta’s EnCodec or others) as they become available. The separation feature will allow a jazz student, for example, to isolate the piano from a recording to hear it more clearly, or remove vocals from a track to practice singing. Ensure the system can handle common audio formats and sample rates, and optimize the separation process (possibly by preprocessing audio into chunks or using GPU acceleration on the server).
	•	Automatic MIDI Transcription (Jazz-Focused): Implement automatic transcription of audio into MIDI using an open-source model, and fine-tune this model for jazz music. There are existing models such as Google Magenta’s Onsets and Frames (for piano transcription) and Spotify’s Basic Pitch, which is a lightweight, instrument-agnostic audio-to-MIDI converter ￼. We can start by integrating one of these models to convert a monophonic or polyphonic audio recording into a MIDI file (and/or MusicXML for sheet music). To make the transcription jazz-aware, we will fine-tune or retrain the model on datasets of jazz recordings and their transcriptions. Jazz music has nuances like swing rhythm, syncopation, and extended chords that generic transcription models (often trained on classical or pop data) might not capture well. By training on jazz-specific datasets (for example, transcribed solos or the PiJAMA dataset of jazz piano performances), we ensure the model better represents jazz idioms ￼. The output for users will be a MIDI file representing the notes of their track. In the UI, users could see a piano-roll view or musical notation of the transcription, and they should be able to listen to the MIDI playback to verify accuracy. This transcription feature is key for musicians who want to analyze solos or transcribe chords from recordings.
	•	Music Theory Analysis with Music21: Extend or integrate the Music21 library to perform advanced music theory processing on the transcribed MIDI data. Music21 is a Python toolkit for computer-aided musicology that provides utilities for analyzing musical structure, chords, scales, and more ￼. We will use Music21 (possibly running on the server side) to parse the MIDI and extract higher-level musical information. For example, determine the key of the piece, chord progression (with roman numeral analysis or jazz chord symbols), detect sections or repeating motifs, and identify scale/mode usage. We might need to extend Music21 with jazz-specific theory rules – for instance, recognizing tritone substitutions, modal interchange chords, or altered dominants common in jazz. The system can present analysis insights to the user, such as “This section is a ii-V-I in G major” or “The chord in bar 8 is a G7 altered chord.” These insights help users understand the theory behind the music. Implementing this may involve writing custom analysis scripts using Music21’s framework, and ensuring that the analysis runs automatically after transcription.
	•	Sheet Music Display & MIDI Playback: Provide options for users to view the transcribed music as sheet music and to playback the MIDI. Once the MIDI is obtained from the transcription model, we can convert it to a notation display – possibly by generating MusicXML and using a library or component to render sheet music in the browser (there are JavaScript libraries like VexFlow or OSMD for rendering MusicXML/MEI as notation). This will allow users to see standard notation for the separated parts or the full transcription. Additionally, allow the user to listen to the MIDI using a basic synthesizer. This could be as simple as a piano sound playback using the Web Audio API (for example, using Oscillators for sine wave tones or a soundfont sample for a piano). A basic synth playback gives immediate auditory feedback of the transcribed notes without needing any external MIDI device. Users might toggle between hearing the original audio and the MIDI version to gauge transcription accuracy. Ensure the timing alignment is maintained so the MIDI playback syncs with the original if played together. This feature turns the system into a transcription playback tool – useful for music students who want to follow along with sheet music as the song plays.
	•	Downloadable Outputs: For convenience and further use, allow users to download the results of the analysis. This means providing download links for the separated audio stems (e.g., a ZIP file containing individual WAV/MP3 files for vocals, bass, drums, etc.), the transcribed MIDI file (.mid), and the sheet music or MusicXML file. This way, users can take the outputs into their own DAW (Digital Audio Workstation) or notation software for editing or practice. The system should generate these files on demand and make sure they are properly labeled and formatted. For example, stem files could be named “songname_drums.wav”, “songname_bass.wav”, etc. Ensuring correct timing and length on these exports is important (e.g., include a count-in or silence so they all align if played together). Providing these downloads adds a lot of value to the user, as the tool not only analyzes music but also produces artifacts the user can keep.

Phase 3: Live Input & Real-Time Music Analysis

Phase 3 moves the system beyond offline processing into real-time interaction. The goal is to allow a user to play an instrument (or sing) live into the system and have the software analyze the performance on the fly, as well as provide immediate accompaniment. This phase is all about ultra-low latency processing – the system should feel responsive, as if playing with a human accompanist or using a hardware effects pedal. New features and technical components in this phase:
	•	Live Audio Capture: Implement the ability to capture live audio input through the microphone or an instrument line-in. In a web context, this means using the Web Audio API’s getUserMedia to access the audio stream from the user’s device (or using native code if this were a desktop app). The UI will have a “Live Mode” or “Jam Mode” where the user can arm the system to start listening. It should support common scenarios like a guitarist using an audio interface or a horn player using the computer’s microphone. We must handle input level monitoring and possibly provide a calibration or level meter so the user knows their signal is being received. The captured audio then needs to be fed into our analysis pipeline in real time.
	•	Real-Time Processing Pipeline: Develop a real-time audio processing pipeline optimized for minimal latency. This likely involves using an AudioWorklet node (in the browser) or a real-time audio thread (in native code) to process audio in small buffer chunks (e.g., 128 samples or less, which is a few milliseconds). The pipeline will take incoming audio frames and perform two main tasks almost concurrently: (1) Transcription/Analysis of the input, and (2) Generation of accompaniment audio. For the transcription part, we might use a lightweight, streaming version of the transcription model from Phase 2 or a simpler algorithm for real-time note detection. Techniques like onset detection and pitch tracking can be used frame-by-frame. For example, use a pitch detection algorithm (like YIN or CREPE) to detect the fundamental frequency of notes in real time, and onset detectors to know when new notes start. These can feed into a rolling MIDI-like transcription of what the user is playing. This needs to happen with only a few milliseconds of delay. On the accompaniment generation side, the system should, in parallel, determine the appropriate backing (e.g., a chord or bass note to play along) based on the user’s recent notes or the known chord progression of the tune being played. All processing should be done in a streaming manner without large buffering. To achieve this, heavy ML models might be pruned, quantized, or run on a WebAssembly thread. If needed, we might delegate some tasks to a server optimized for low latency, but the round-trip time might be a limiting factor, so the preference is on-device processing. The real-time pipeline will likely be the most technically challenging part, requiring careful optimization and possibly using multi-threading (Web Workers or Worklets) so that analysis doesn’t block audio output.
	•	Real-Time Transcription & Musical Analysis: As the user plays, the system transcribes the notes in real time and performs basic musical analysis on them. For the scope of this phase, the transcription might not be 100% precise or complete (since high-accuracy transcription can introduce latency), but it can capture essential information like the pitch and timing of notes or chords being played. If the user is improvising, the system could detect the key or scale in use on the fly, or at least track the chord progression if one is being followed. The musical analysis might be simpler here than in offline mode – perhaps focusing on things like current chord estimation or tempo detection. For instance, as a jazz musician plays a progression, the system could infer “the user seems to be outlining a Dm7 chord resolving to a G7” and use that knowledge for accompaniment. All of this analysis must update continuously as new audio comes in, effectively providing a stream of musical metadata about the user’s playing.
	•	Basic AI-Generated Backing (Bassline): Enable the system to play a backing track in real time to accompany the user’s live input. In this initial live phase, we will focus on a simple form of accompaniment: a bassline. The idea is to have the system function like a live rhythm section partner, starting with the bass player role. Using the musical analysis of the user’s input (or a predetermined chord progression if practicing a specific tune), the system generates a bassline that fits. For example, if the user is practicing a jazz standard (like a 12-bar blues or a tune from the Real Book), the system can have a predefined or algorithmically generated walking bass pattern for those chords. The bassline can be generated using rules from jazz theory (walking bass typically uses quarter notes, approach tones, scale tones outlining the chord) or even a simple AI model trained on basslines. Initially, this could be rule-based: e.g., cycle through arpeggios or scale patterns appropriate to the current chord, ensuring the last note of each bar leads chromatically into the next chord’s root. The generated bass MIDI notes would then be synthesized to audio (using a bass instrument soundfont or a simple synthesized bass tone) and output via Web Audio with very low latency, so it aligns with the user’s playing in time. The user should have the experience that as soon as they play, a bass accompanies them in the correct key. We should also handle tempo synchronization – either have the user count in or tap a tempo that the system follows, or dynamically adjust timing to the user if possible. Starting with bass ensures we handle the fundamental harmonic support; later phases will add more complex accompaniment. This feature transforms the application into an interactive jam tool, where a solo musician can get immediate backing, making practice more engaging.

Phase 4: Advanced AI-Generated Accompaniment

By Phase 4, the system will evolve into a sophisticated AI accompanist capable of providing a full-band experience. This phase expands the accompaniment beyond just a bassline to include chords (e.g., piano or guitar comping), drums, and potentially other instruments, creating a richer backing track. The key challenge is to make the AI-generated accompaniment dynamic and context-aware – it should react appropriately to the live performer’s playing in real time. Achieving this will likely involve more advanced AI models and careful optimization. The plan for this phase includes:
	•	Full Rhythm Section Generation: Build upon the bass accompaniment and introduce chordal accompaniment and drums. For chords: implement an AI “comping” partner (piano or guitar) that plays chord voicings and rhythms in a jazz style according to the chord progression and the live input from the user. This could be done by training a model on jazz comping patterns or by rule-based methods (e.g., a library of voicings and a rhythmic template for comping). For drums: include a virtual drummer that can keep time (with a swing feel for jazz) and add rhythmic support (hi-hat, ride patterns, snare accents, etc.). Initially, drum patterns could be straightforward (like a standard swing rhythm: ride cymbal on every beat with occasional snare hits on 2 and 4, for example) and then potentially become more adaptive. The combination of bass, chords, and drums means the user will hear a trio backing them up. The system might allow toggling these on/off (for instance, practice just with bass and drums or full trio). We will likely use MIDI generation for these parts and have pre-loaded sound samples or a synth for each instrument to output audio. The arrangement generation might use generative models: for example, a Transformer-based model that generates chord comping given the chord sequence, or a recurrent model that outputs drum hits given a style. The focus is on style and appropriateness — the accompaniment should not be random, but follow jazz conventions (e.g., a comping pattern that leaves space during a soloist’s phrase, a drummer that does a fill at the end of 8 bars, etc.). This will significantly enhance the musicality of the session, making the AI feel more like a real band.
	•	Context-Aware Generative AI (Reinforcement Learning & Dynamic Response): To make the accompaniment respond to the human musician in real time, we will explore advanced techniques like reinforcement learning (RL) and interactive generative modeling. One approach is to train a reinforcement learning agent for accompaniment, as researchers have started to do for interactive music systems. For instance, the RL-Duet project demonstrated an RL-based model that can listen to a human melody and generate a fitting accompaniment sequentially ￼. We can design a similar RL framework where the AI accompanist observes the user’s playing (state) and chooses musical actions (notes/chords/rhythms) that maximize a reward function (the reward could measure musical coherence, staying in key, following rhythm, etc.). Over many simulations or training sessions (possibly using recorded solos as the “human” and known good accompaniments as ideal outputs), the AI learns to adapt its playing to complement the soloist. Besides RL, we can also use supervised generative models that take into account context: for example, a neural network that at each time step takes the last few seconds of the user’s notes plus the last few seconds of its own accompaniment, and outputs the next accompaniment chunk. Such a model could be trained on duet or band recordings where one part is treated as “solo” and the others as “accompaniment”. The generative model (be it an LSTM, Transformer, or a newer music ML model) should be designed for fast inference to be usable live. We might employ techniques like sequence truncation (only looking at a short recent window of notes to decide the next output) to limit computation. The result aimed for is an AI that, for example, can catch if the user starts playing louder or more aggressively and then the comping intensifies, or if the user takes a rubato tempo, the AI adjusts timing accordingly. This context awareness and reactiveness is the hallmark of a good human accompanist, and we want to approximate that behavior.
	•	Real-Time Performance Optimizations: With multiple AI components generating music at once, we must heavily optimize for real-time performance. This involves using faster inference methods for neural networks, such as quantizing models to reduce computation (e.g., 8-bit integer math instead of 32-bit floats) and using model distillation to make smaller networks that run quicker. If the environment allows, leverage GPU acceleration for model inference – for example, in a browser context, use WebGPU or WebGL shaders for parallel processing, or if on a native app, directly use the system’s GPU or dedicated ML hardware. We should also pipeline the processing: different threads or worklets could handle different parts of accompaniment (one for drums, one for chords, etc.) so they run in parallel and then mix the outputs. Ensuring thread-safety and timing alignment will be important when doing this. Another angle is to pre-generate as much as possible: for example, if the chord progression is known, the system could precompute some patterns or embeddings that make on-the-fly generation easier. We will also look into reducing latency in every part of the chain – from audio input capture (minimize buffer sizes), to processing (optimized code, possibly using WASM as in earlier phases), to output (schedule sounds with precise timing using Web Audio API’s Clock). By the end of Phase 4, the system should be capable of producing a full, rich accompaniment in sync with a live musician with minimal lag. This will have transformed the application into a true real-time AI jam session platform.

Phase 5: Song Catalog & Expansion

In Phase 5, the project will incorporate a library of songs and specialized practice tools to broaden its usefulness and user base. The emphasis here is on content (especially jazz standards) and learning features, turning the system into not just a reactive tool but also a proactive educational platform. The key additions in this phase include:
	•	Built-In Song Catalog (Jazz Standards): Develop a catalog of songs within the app, starting with popular jazz standards commonly found in the Real Book (a famous collection of jazz lead sheets) ￼. Each song entry would include metadata such as the title, composer, the chord changes, and possibly a reference tempo and style. By having songs built-in, users can quickly select a tune they want to practice or analyze without needing to upload audio. We can use the chord progressions from the Real Book and perhaps provide backing tracks generated by the AI for each. For example, a user could select “Autumn Leaves” from the catalog and immediately get a backing track (bass + drums + comping) for that song, with the chord progression known to the system. The melody could also be included as MIDI for reference (or even displayed as sheet music for learning the head). We should ensure proper licensing or use public-domain jazz tunes as needed, but many jazz standard progressions can be used educationally. The song catalog will allow structured navigation – users can browse or search tunes, and filter by key, difficulty, etc. Over time, the catalog can expand beyond jazz to other genres, but jazz standards will be our starting point due to their pedagogical importance and the fact that their structures are well-defined.
	•	Structured Learning & Practice Modes: Introduce features that help users practice improvisation and develop their skills using the AI accompaniment. One idea is a Practice Mode where users can set up specific practice routines (for instance, looping a ii-V-I progression in different keys, or trading fours with the AI drummer). The system can have exercises like: “Play the arpeggios of each chord” and it could listen and validate the user’s input using the transcription engine, giving feedback if needed. Another feature might be Improv Training, where the AI can take a solo for a chorus to demonstrate, then leave space for the user to solo in the next chorus (essentially call-and-response or trading solos). Because the AI can also generate improvisation, the user can learn by example. We could implement a mode where the AI plays a sample improvised phrase and the user tries to imitate it, with the system comparing the user’s attempt via transcription. Additionally, provide tools like a metronome or the ability to slow down/speed up the backing track without changing pitch (time-stretching the AI output) so that learners can start slow and build up speed. Over time, user progress can be tracked – e.g., how long they practiced, which songs, perhaps even analyze their solos for complexity growth. These learning features will position the system as not just a toy, but a serious practice companion for music students.
	•	User-Customizable Playback Settings: Allow extensive customization of the playback and AI behavior to suit individual practice needs. Users should be able to adjust the tempo of the backing tracks, change the key of the song (with the AI accompaniment transposing accordingly), and select which instruments in the accompaniment are active (maybe practice with just drums and bass, or solo with only piano comping). Importantly, expose controls for the AI improvisation and style settings. For example, a user might set the accompaniment “complexity” level: at a low setting, the AI plays very simple bass lines and comping (good for beginners or when the user wants minimal interference), while at a high setting, the AI might add more passing tones, syncopation, and rhythmic hits (challenging the user with a more interactive experience). There could also be a setting for the AI’s soloing – e.g., a toggle for “AI takes solos” which, if enabled, the AI will occasionally improvise a solo over the changes (letting the human practice comping, for instance). Style presets can be included: swing, bossa nova, uptempo bebop, etc., so the user can choose the vibe of the accompaniment. All these options make the system flexible for various use cases: from slow practice to realistic jam session simulation. On the technical side, implementing this means parameterizing our AI models (e.g., a randomness/creativity parameter for the generative models to make them more or less “busy”) and possibly having multiple trained profiles for different styles. Providing a user-friendly settings panel in the UI with sliders and dropdowns for these will empower users to tailor the AI to their needs. By the end of this phase, the system is not only powerful in analysis and accompaniment, but also rich in content and adaptability, making it a comprehensive platform for music practice and exploration.

Additional Notes

Throughout all phases of development, several important technical considerations and best practices must be kept in mind to ensure the system performs well and provides a great user experience:
	•	End-to-End Low Latency: Low latency is a recurring requirement, especially for real-time features (Phases 3 and 4). We must use techniques at every step to minimize delay. This includes choosing the right buffer sizes for audio processing (smaller buffers reduce latency at the cost of higher CPU usage), using real-time safe programming practices (avoiding garbage collection or memory allocations in the audio callback, for instance), and possibly leveraging WebAssembly or native code for performance-critical sections. WebAssembly, in particular, allows us to run DSP and even certain ML tasks in the browser at near-native speed ￼. If any processing is done server-side (like heavy separation in Phase 2), we should hide that latency by doing it asynchronously (not blocking the UI) and possibly using background processing. For live audio, everything should ideally happen on the client machine to avoid network latency. We’ll also investigate audio threading (using Web Audio API’s AudioWorklet) to ensure timing precision. By profiling and testing the system with real instruments, we can identify bottlenecks and optimize (e.g., using a faster algorithm for a certain analysis if it’s causing delay). The goal is to make the interactive experience feel instantaneous and natural.
	•	Flow-Based Models for Separation: When working on source separation (Phase 2), consider the advantages of flow-based machine learning models like Glow for source separation. Traditional separation (like Demucs) directly learns to extract sources, but a flow-based approach learns a probability distribution of each source conditioned on the mix. Research has shown that training a Glow model for each instrument (source prior) can be effective in separating music by optimizing a likelihood objective ￼. While implementing a full Glow-based separation might be complex, being aware of it means we can design our separation module to be swappable or upgradable. Perhaps in a later iteration, we could incorporate a pre-trained flow model to improve separation quality or reduce artifacts. Additionally, flow models can sometimes be inverted in real-time since they are typically fast to sample from, which might lend themselves to quicker processing once trained. The architecture should remain modular enough that the source separation component can be updated from one model to another without affecting the rest of the system.
	•	Jazz-Specific Training for Transcription: To ensure high-quality MIDI transcription and analysis, we need to train or fine-tune our models on jazz-specific datasets. As noted earlier, much automatic transcription research is biased towards classical piano or other instruments ￼, so models out-of-the-box might not handle swing rhythms or complex chords well. By curating a dataset of jazz audio with corresponding MIDI (or at least using something like the Weimar Jazz Database or transcriptions of jazz solos), we can fine-tune the transcription model to improve its jazz vocabulary. This will improve things like recognizing triplet swing eighths (instead of straight eighths), identifying extended chord tones or grace notes in solos, and notating rhythms in a way jazz musicians expect. We should also test transcription on common jazz pieces and compare it against known transcriptions to evaluate accuracy. If needed, we can incorporate user feedback or correction in the training loop – for example, if users fix the transcribed notes via the interface, those corrections could be fed back (with permission) to continually refine the model. The emphasis is that the system should “understand” jazz as much as possible, which will reflect in all parts – transcription, analysis, and accompaniment.
	•	Integration with DSP Frameworks (Faust/JUCE): To accelerate development and ensure robust audio processing, we should leverage existing audio DSP frameworks where possible. Faust (Functional Audio Stream) is a functional programming language for audio DSP that can compile code into C++, JavaScript, WebAssembly, and more ￼. We could write certain audio effects or analysis components in Faust and then easily get a WebAssembly module or even a native plugin from it. For example, if we need a specific audio effect (like reverb or EQ for the separated stems, or a special synthesizer for the accompaniment), Faust could provide an efficient implementation quickly. It’s well-suited for creating filters or other DSP routines with high performance. JUCE is a C++ framework commonly used for audio applications and plugins. While JUCE is more relevant for native applications than web, we might utilize it if we develop a desktop version or a plugin version of this system in the future. JUCE could help with cross-platform audio I/O, GUI components, and it has a lot of built-in DSP utilities. It’s also possible to use JUCE’s DSP code and then compile to WebAssembly since JUCE is C++ (though not trivial). Considering these frameworks in our design means we write less low-level code from scratch. For instance, instead of hand-coding a synthesizer in JavaScript, we might write it in Faust and compile to Web Audio. Or use JUCE’s well-tested algorithms for audio file decoding, etc. However, we must also balance using these tools with the need for web integration (Faust is quite web-friendly via wasm, JUCE less so). Overall, exploiting such frameworks can reduce development time and yield high-performance, reliable components for our system.
In summary, this roadmap outlines a step-by-step development plan, from building the foundational UI and architecture (Phase 1), through adding sophisticated audio analysis (Phase 2), moving into real-time interactive functionality (Phase 3 and 4), and finally enriching the system with content and learning features (Phase 5). By following these phases and keeping the technical considerations in mind, we aim to develop an AI-powered music analysis and accompaniment platform that is powerful, musician-friendly, and particularly tailored to the needs of jazz improvisers. Each phase builds on the previous ones, gradually transforming the system from a static analysis tool into a live interactive band-in-a-box with pedagogical value. The result will be a unique application that leverages cutting-edge AI and audio technology to assist musicians in both understanding music and practicing it in an engaging way.